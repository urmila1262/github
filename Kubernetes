What is Kubernetes?

Kubernetes is a container management or orchestration tool.It is also known as K8s, its a open source and written in GO language.
container orchestration tool or engine automates deploying,scaling and managing containerized application on a group of servers.
ex: Kubernetes,docker swarm,Apache Mesos Marathon.

Kubernetes --->take care of deploying,scaling,scheduling,load balancing,batch execution,rollbacks and monitoring.

1) Automatic bin packaging: Kubernetes automatically packages your application and schedules the container based on the requirements and resources available.
Pods and Nodes: containers are placed in functional unit called pod and pods are housed inside Nodes
2)Service discovery and load balancing:
How kurbernetes organizes containers
Kubernetes doesn't run containers directly instead it wraps one or more containers into a high-level structure called pod.Each pod contain a storage volume and 
every pod has a unique network IP.
All pods having same set of functions are abstracted into sets called services and this services has given a DNS name with this system Kubernetes has control over 
network and communication between pods and can load balance accross them.
3) Storage Orchestration:
Containers running in the pods need some storage but a single storage volume present in each pod for all containers.
Kerburnetes allows to mount the storage system of your choice like local,cloud(AWS),NFS(Network file system).
4) Self healing:
If a container fails Kubernetes restarts the container,if node dies - replaces and reschedule containers on other nodes,if container doesn't respond to user 
defined health check - kill container. The process that takes care of all this is replication controller.


## launch.sh --->it will launch the cluster
## kubectl cluster-info   --->it will give kubernetes master and DNS URL and port no(Health check)
## kubectl cluster-info dump  ---> to further debug and diagnose cluster problems
## minikube version
## minikube atart --wait=false
## kubectl cluster-info  --->cluster info
## kubectl get nodes -->to view nodes in the cluster.
## docker -v --->docker version
## kubectl run ----> it allows containers to be deployed onto the cluster
## kubectl create deployment first-deployment --image=katacoda/docker-http-server  ----->to create the deployment
## kubectl get pods  ----> to check status of deployment we use this command
## kubectl expose deployment first-deployment --port=80 --type=NodePort   ---->once the container is running it can be exposd via diff networking options, depending on requirements.
One possible solution is NodePort, that provides a dynamic port to a container.

## export PORT=$(kubectl get svc first-deployment -o go-template={{range.spec.ports}}{{if.nodeport}}{{"\n"}}{{end}}{{end}}')
## echo "Accessing host01:$PORT"
Accessing host01:30340
## curl host01:$PORT
<h1>This request was processed by host: first-deployment-666c48b44-4kb2r</h1>  ----------->All this above commands finds the allocated port and executes a HTTP request.

Enable the dashboard
## minikube addons enable dashboard
## Kubectl apply -f /opt/kubernetes-dashboard.yaml --->we will get msg like service/kubernetes-dashboard-katacoda created.


What is K8s?
Kubernetes is open source container orchestration tool developed by google helps you to manage containerized applications in different deployment environments 
like physical,virual or cloud machines.

What features does orchestration tool offer?
1)High availability or no downtime(always accessible by users)
2)Scalability or high performance
3)Disaster recovery - backup and restore

Terms in K8s:
Node:Node is a physical or virtual machine.

Pod:Smallest unit of k8s and abstraction over a container creates running environments over containers or images.Each pod gets its own IP Address and it is internal
    to communicate with other pods. Pods will die easily when containers,application or node is crushed and new pod will be created with new IP address.
    
Service:Service is having static IP Address and each pod has service, even if pod dies the service and its IP remains same.

Ingress:In Kubernetes, an Ingress is an object that allows access to your Kubernetes services from outside the Kubernetes cluster. You configure access by creating 
    a collection of rules that define which inbound connections reach which services. This lets you consolidate your routing rules into a single resource.(routing
    Traffic to pods).
    
ConfigMap: External configuration for your app like DB etc and it is connected to the pod so that when ever pod requires DB it will get from configmap. If there 
   is any change in name or URL of DB then simply change in configmap helps app to access new DB.
   
Secret:Secrets came into picture becoz we can't store pwd of DB in configmap, so in secrets we put our DB credentials and which is encoded in base64.Just like
  configmap we can also connect secret to the pod so that pod can access data in both by using environmental varables or as a properties file.
  
Volumes:If DB pod or service get restarted our data will gone so Volumes came into picture.Volume is physical storage on local machine or remote outside of k8's 
  cluster(may be cloud or your own storage).k8's doesn't maintain any data persistance.
  
Replica:Suppose if the pod crushes or restarts in production it will effect our operations so,we need to maintain replica of pods with same service and it also 
  helps in load balancing in operations.For this replica we can use blueprints of my-app pods this is deployments.
  
Deployment:We can deploy the pods for replica used for loadbalancing so we can scale up and down the replicas.

Statefulset:In case of DB pods it will replicate Storage space but not data in it and also we can access to read or retrive data so we use Statefulset for DB and 
  other stateful apps.(A stateful app is a program that saves client data from the activities of one session for use in the next session.)
  
  
  Kubernetes Architecture:
  
  One of the main components of Kubernetes is Node,each node has multiple pods running on it.
  Three processes that must be installed on every Node that should schedule and manage pods.Nodes are cluster servers that actually do work.
  
  First process that need to run on each node is
  
  1)Container runtime: (Docker or any other containers)You need to install a container runtime into each node in the cluster so that Pods can run there. 
    This page outlines what is involved and describes related tasks for setting up nodes.
    
  2)Kubelet: The kubelet is the primary "node agent" that runs on each node and it interacts with both container and node.Kubelet is responsible to run the 
    pod with a container inside by taking configuration and assigning resources from container to the pod like cpu,storage resouces.
    
 3)Kube proxy:It will forwards the requests(communication between node)
    kube-proxy is a network proxy that runs on each node in your cluster, implementing part of the Kubernetes Service concept. 
    kube-proxy maintains network rules on nodes. These network rules allow network communication to your Pods from network sessions inside or outside 
    of your cluster.
    
 How do you interact with this cluster?
 How to:
    Schedule pod?
    Monitor?
    Reschedule/Restart pod?
    Join a new node?
  All above processes are controlled by Master processes which will take care of all the nodes.
  
  Four processes run on every master node that controls cluster state and worker nodes as well
  
  1)API server: To deploy a new application in a kubernetes cluster, interact with API server with some client.API server is like cluster gateway which gets 
  initial requestes for any updates into the cluster or any queries from the cluster and also it acts as a gate keeper authentication to make sure that an
  authorized request came to the cluster. When you want to Schedule new pods, deploy new apps, create new services you need to interact with API server on 
  the master node then API server validate your request if everything is fine it will forward your request to the other processes inorder to schedule pod
  or other components that you requested. It is good for security purpose becoz it has one entry point to the cluster.
  
  2)Scheduler: if you send request to create new pod the request will send through API server to Scheduler, Scheduler knows where to create pod(on which worker
  node) and how much CPU,memory required.Scheduler just decides on which node new pod should be scheduled, sends request to kubelet, it will do actual 
  configuration of CPU,memory etc and executes the process.
  
  3)Controller manager: Detects when pod dies or cluster state changes and send request to the scheduler again same process repeats it will check which node 
  is having more free space, then it will restart the pods or schedule to create new pods. 
  
  4)etcd: It is a key value store of cluster simply called as cluster brain,what ever the changes(suppose pod dies,new pod scheduled etc) occur in the cluster
  that will be stored in the etcd, but application data is not stored in the etcd. It will store only data related to cluster master node.
  
  Minikube:
  Minikube is a master node cluster where masternode and worker node both run on one node.In this node the docker container is pre-installed so you can run 
  containers or deploy the containers. It will run on your laptop as a virtualbox, basically creates virtualbox on your laptop the node runs on that virtual box.
  To summerize minicube is a one node cluster that run in a virtualbox on your laptop used to test kubernetes on local setup.
  
  Kubectl:
  Which is a command line tool for kubernetes cluster to create nodes and services.
  First we need to speak with master process and one of the master process in minikube is API server is actually main entry point to the kubernetes cluster.
  We need to talk to API server and the way to talk to API server are many way through UI,kubernets API,CLI called kubectl and kubectl is the most powerful
  among three clients. Kubectl used to create pods,create services and destroy pods etc.
  Kubectl is not only used to interact with minicube but also used for cloud clusters.
  
  >brew update
  >brew install hyperkit
  >brew install minikube
  >kubectl
  >minikube
  >minikube start --vm-driver=hyperkit --->saying minikube to use hyperkit hypervisor
  >kubectl get node --->gives status of running nodes
  >minikube status --->we can get status of minikube,kubectl,APIserver running or not and kubeconfig -configured.
  >kubectl version --->to know the version of client and server.
  >kubectl get deployment --->to show deployments
  >kubectl get pod --->status of pods
  >kubectl get services --->status of services or list any kubernetes components
  >kubectl create -h --->help for kubectl create command(list of components to create)
  Note: Pod is a smallest unit we are not creating pods directly but there is an abstraction layer over it called deployment.
  
  >kubectl create deployment NAME --image=image [--dry-run] [options]
  >kubectl create deployment nginx-depl --image=nginx --->creates latest nginx container from docker hub
  >kubeclt get deployment --->gives status of deployments.
  
  >kubectl get pod --->to see the pods with status container creating,running
  Note: Deployment has blueprint to create pod, most basic configuration for deployment(name and image to use)
  
  >kubectl get replicaset ---> this is another component which will come by default(Replicaset is managing the replicas of pods)
  Note: Pod will have the hash of replicaset and pods hash ID. We can't create,delete replicas just deploy, for more replicas we have options. 
  
  >kubectl edit deployment nginx-depl ---> directly editing pods(Auto generated configuration file with default values)
  Note: We can change in this configuration file in pods and save the changes it will update with new pod
  
  >kubectl get deployment ---> new deployment is available
  >kubectl get pod --->old(terminating) and new pods are shown but new pod is running.
  >kubectl get replicaset --->old one has no pods it in and new has pods
  >kubectl describe pod nginx-depl --->gives all details of the pod.
  
  Debugging pods:
  >kubectl logs nginx-depl-5c8bf76b5b-pn2kq --->gives the logs related to pods
  >kubectl exec -it {pod-name} -- bin/bash ---> To get the terminal of the pod(to get inside pod we will get shell - as admin), exit to come out from terminal.
  

  create mongo deployment
>kubectl create deployment mongo-depl --image=mongo
>kubectl logs mongo-depl-{pod-name}
>kubectl describe pod mongo-depl-{pod-name}

delete deplyoment
>kubectl delete deployment mongo-depl --->delete the pods mango db.
>kubectl delete deployment nginx-depl

create or edit config file

>vim nginx-deployment.yaml

-------
apiVersion: apps/v1
kind: Deployment   ----->what to create
metadata:
  name: nginx-deployment ---->name of the deployment
  labels:
    app: nginx 
spec:      ---->Specification of deployment
  replicas: 2 ---->how many replicas
  selector:
    matchLabels:
      app: nginx
  template:    ----->template has metadata and its own spec here pod is defined
    metadata:
      labels:
        app: nginx
    spec:      ------>specifications of pod(blueprint for the pods)
      containers:
      - name: nginx ----->name of 
        image: nginx:1.16
        ports:
        - containerPort: 8080 ----->on what port pod need to run

-----
>kubectl apply -f nginx-deployment.yaml ----> nginx deployment created.

>kubectl get pod ----->we can see pod created and running
>kubectl get deployment. ---->deployment created few secs ago
Note:We can change the file configuration if any extra replicas required, then again run apply command.

YMAL configuration file in kubernetes
overview:
-The three parts of configuration file
-Connecting deployments to service to pod
-Demo

3 parts are:

1) Metadata :Name of the component
2)Specification: every configuration is defined here(attributes inside spec are specific to kind -deployment or service).
first two lines gives us what is kind of component and apiversion will change for every deployment and service.
3)Status:Automatically generated and added by kubernets.k8s always compairs (desired state # actual state) if it doesn't match then k8s try to fix it becoz of
its self healing nature.It will check in config logs status and find any diff in status and specification it will adds status, updates frequently. k8s get this 
information from etcd. etcd holds the current status of any k8s components.


nginx-service.yaml

--------

apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  selector:
    app: nginx
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8080

--------

Formate of configuration file:

- .yaml extention 
- syntax: it is strict with syntax. (yaml validators online to fix syntax)
- store the config file with your code or to have its own git repository.

Connecting components:

Labels and Selectors
metadata part contains labels and spec part contains selectors

metadata -- any key value pair for componenet

-------
labels:
 app:nginx  ---> here we have nginx
-------
 
 -pods get the label through the template blueprint.
 -This label is matched by the selector
 
 -------
  selector:
   matchLabels:
    app:nginx
 -------

>kubectl apply -f nginx-deployment.yaml
>kubectl apply -f nginx-service.yaml
>kubectl get pod
>kubectl get service ---->it will shows services created and with the defined name,port etc.
>kubectl describe service nginx-service
Note:here we get info related to service and endpoints are also found where IP addresses of pods under that service are defined.
>kubectl get pod -o wide ----> here we get more info IP address also present.

Status Automatically generated:
>kubectl get deployment ngnix-deployment -o yaml ---->will get the updated configuration of deployment which result in the etcd of cluster for every component.
>kubectl get deployment ngnix-deployment -o yaml>nginx-deployment-result.yaml ---->saving output in the file

delete with config
>kubectl delete -f nginx-deployment.yaml ----->to delete the deployment.
>kubectl delete -f nginx-service.yaml

Complete Application setup with kubernetes components:
mangodb,mangoexpress
we can create mangodb pod and create internal service to the pod so that only internal requests take to the pod. 

Then we create mango express  
1)give URL of mango db so that mango express will connect to DB
2)mango db username and pwd so that mango express will authenticate to DB
Note: We can pass this data to mango express with deployment.yaml configuration file by declaring some variables. We have configmap which contains URLs and 
Secrets contains credentials we have referance for this in deployment file. Now mango express access this through browser in order to do this we have an external
service that allow external request to talk to the pod.
URL:
-Ip address of node
-Port of external service.

Browser Request flow through the k8s components:
Request starts from browser --->it will connect with mango express external service ---->sends it to mango express pod --->then internal service of mango DB
(db URL) --->then it will send to mango db pod where it will authenticate the request using credentials.

>kubectl get all --->it will get all components running in k8s

nodePort:must be in the range of 30000 to 32767 --->this port no is given in browser to access external service

>minikube service mongodb-express-service --->displays name,url with publice IP and external port as specified to open in browser.

Kubernetes namespaces:
- What is a Namespace?
- What are the use cases?
- How namespaces work and how to use it?

In k8s we can organise resources in namespaces, there are many namespaces in kubernetes cluster.We can say name space is virtual cluster inside cluster.
k8s provides default namespaces
>kubectl get namespace ---->gives names of default namespaces

5)kubernetes-dashboard is only with minikube it is not installed in cluster.
1)kube-system namespace --->is not ment for you basically you should not create or modify in kube-system. The components that are defined in kube-system are system
processes they are from master managing process or kubectl etc.
2)kube-public namespace contains publicly accessible data, it has a configmap which contains cluster information so it is accessible without authentication.
>kubectl cluster-info ---> gives info is control panel, kube DNS are running.
3)kube-node-lease ---> hold info of heartbeats of nodes, each node has assiociated lease object in namespace, determine the availability of a node.
4)Default namespace ---> To create resources at the begining if you haven't created a new name space. You can add and create new namespaces.

>kubectl create namespace my-namespace ---> to create namespace.
>kubectl get namespace --->we can see new namespace in the list.

what is the use of namespaces?

1)Create namespaces with configuration files:
If we create all our resources in default namespace it will be hard to find the specific resouces belongs to so, we can create our own namespaces to avoid the
confusion,ex:create db namespace and add all resources related to DB, like monitorning, elastic stack, nginx-ingress name space etc logically grouping the 
resources inside the cluster, it is not suggested for smaller projects contains lessthan 10 resources.

2)conflicts:
If two teams are using same cluster they can deploy same app so that the 2nd app deployment overwrite 1st app deployment. Inorder to avoid the conflicts 
in cluster give two namespaces for each team to aviod overwiting even if the app name is same. If they are using jenkins automation tool for deployment 
they not even knows they have overwritten.

3)Resource sharing: staging and deployment:
If two staging and deployment or (production blue and production green) are using same Nginx-ingress controller,Elastic stack then we can place in diff 
namespace and share them two both staging and deployment.

4)Limit resources and access to namespaces when you are working with diff namespaces:
If two teams are working on two diff namespaces in the same cluster, grant permissions only to the respective namespaces avoiding the teams to access and 
change other teams deployments.Each  team has it own,isolated environment.Limit CPU,RAM,Storage per namespaces.

Characteristics of namespaces?

1)You can't access most of the resources from another namespace. Each NS must have its configmap and secrets.
2)Service in another NS can be used by the other NS
3)Components, which can't be created within a NS, they live globally in a cluster and you can't isolate them like volume,node etc
>kubectl api-resources --namespaced=false ---> to list the isolate resources or not in NS in cluster.
>kubectl api-resources --namespaced=true --->resources in NS.

Create componenets in a NS:

------
mysql-configmap.yaml

apiVersion: v1
kind: ConfigMap
metadata:
  name: mysql-configmap
data:
  db_url: mysql-server.database
------

Note: If no NS is defined then the resource will be created in default NS.
>kubectl apply -f mysql-configmap.yaml
>kubectl get configmap
>kubectl get configmap -n my-namespace
>kubectl get all -n my-namespace  --->all resources in NS are listed
>kubectl apply -f mysql-configmap.yaml --namespace=my-namespace --->create configmap in the namespace
>kubectl get configmap -n my-namespace ----> to verify the resource in namespace 

Change active namespace:
To change from default NS to specific NS we need to install an app called "kubens".
>brew install kubectx ---> this can install kubens as well
>kubens ----> gives list of NS and highlight hte one which is active.
>kubens my-namespace ----> to switch the active NS to the specified NS.

Note: Now we can execute Kubectl commands directly without specifying NS as we have used before for default NS.Installation of kubens is differ for OS.


Kubernetes Ingress:

- What is ingress?
- Ingress YAML configuration
- When do you need ingress?
- Ingress Controller

Note: Now we are connecting with an external app through browser by giving IP ,port and http secure protocal, but we can do through "my-app ingress" component
from internal, external service send to ingress and ingress send request to internal service and eventually goes to pod.

exe:
type: LoadBalance
  nodePort: 35010
  
This is for external connection, by providing external port no:

With ingress:

---------
apiVersion: networking.k8s.io/b1betal
kind: Ingress
metadata:
  name: myapp-ingress
spec:
  rules:       --------->routing rules:this host must be forwarding the request to the internal service.User enter browser and ingress define mapping.
  - host: myapp.com   ---->Valid domain address, maps domain name to node's IP address, which is entrypoint.(the node will be inside or outside kubernetes cluster)
    http:   ------->this protocal is not from external path, this is for incoming request gets forwarded to internal service.
      paths:       ----->this is URL path http://domain/path what ever the path after domain that is mentioned here
      - backend:
          serviceName: myapp-internal-service    ----->this is internal service, once ingress maps this is redirected to internal service
          servicePort:8080
---------

How to configure ingress in your K8s cluster?
We need implimentation for ingress which is ingress controller which is pod or set of pods in K8s cluster that evalutes and process ingress rules. The above yaml
file is installed in ingress.

pod<--- app service <---Ingress <----ingress contorller pod

What is ingress controller?

Function of ingress controller is to evalute all rules that you are defined in your cluster to manage all the redirections. This will be the entrypoint to all 
the domains and sub-domains you configured and it will evaluate all the rules in your cluster there are more than 50 rules, which farwording rule applys for that
specific request. k8s Nginx Ingress controller is one of the important controller.

Environment on which your cluster runs:
Cloud service provider like AWS,google cloud ..etc this have out-of-box k8s solutions and have own virtualized load balancer.Here cloud load balancer is there
the external request first hits the load balancer and load balaner send request to ingress controller.

Advantage of cloud provider is you don't have to implement load balancer by yourself it is up and running you can send requset to k8s cluster.
If you runnung k8s cluster in bare metal env you need to configure some kind of entrypoints.Either inside of cluster or outside as separate server you need to 
provide entrypoint and one of those is external proxy server which can be a software or hardware solution that will take the role of load balancer and entrypoint
to your cluster.you have a separate server and you would give IP address and open ports to the request to be accepted and this proxy server will act as 
entrypoint to the k8s cluster and this is the only one server accessing from outside in k8s this is really good security practice.

So, all the request went to the proxy server from there send to ingress controller here it will choose which rule should be taken and whole internal farwording 
will happen.

Configure ingress in minikube:
1)install ingress controller in minikube
>minikube addons enable ingress  ----->this will automatically starts the k8s Nginx implementation of ingress controller 

With one simple command ingress controller will be installed in your cluster 
>kubectl get pod -n kube-system  ---->you will see nginx ingress controller running in your cluster
Now you can create ingress rule the controller can evaluate.
>kubectl get ns ---> to see the k8s dashboard(in minikube dashboard is already out-of-box exist)
>kubectl get all -n kubernetes-dashboard   ------>shows all the components in k8s dashboard

Ingress rule creation:
------
apiVersion: networking.k8s.io/v1betal
kind: Ingress
metadata:
  name: dashboard-ingress
  namespace: kubernetes-dashboard
spec:
  rules:
  - host: dashboard.com
    http:
      paths:
      - backend:
          serviceName: kubernetes-dashboard
          servicePort: 80
------

>kubectl apply -f dashboard-ingress.yaml ----->name of ingress, this command creates ingress
>kubectl get ingress -n kubernetes-dashboard ---->NS name,this command gives us the ingress information.
>kubectl get ingress -n kubernetes-dashboard --watch ---> this will give IPaddress of ingress

Note:We can map this IP address in etc/host file so the the IP address is accessable with some name (dashboard.com this is not specific but here it is host name)
so if we browse dashboard.com will get k8s dashboard.

>kubectl describe ingress dashboard-ingress -n kubernetes-dashboard
this command gives info of ingress here "default backend: default-http-backend:80" so in k8s there is no rule specific to take backend it will be take by default
backend service.

------
apiVersion: v1
kind: Service
metadata:
  name: default-http-backend  --->name of default
spec:
  selector:
    app: default-response-app
  ports:
    - protocol:TCP
      port: 80   ---->port of default backend
      targetPort: 8080
------

Defining multiple paths for same host:
For ex: google is one domain but have multiple services

Multiple sub-domains or domains:


Helm - Package manager of K8s:

- Main concepts of Helm.
- Helm changes a lot between versions
  understanding the basic common principles and use cases when and how we use Helms.
  
What is Helm?

Helm has many functions 

1)is used as package manager for k8s
To package yaml files and distribute them in public and private repositories.
ex: you have deployed your app in k8s cluster and you want to deploy Elastic stack additionally in cluster your app will use it to collect logs. To deploy 
elastic stack in k8s cluster you need some services like stateful set,configMap,secrets,k8s user with permissions,services etc

Helm Charts:
- Bundle of yaml files.
- Create your own Helm charts with helm.
- Push them to Helm repository
- Download and use existing ones

Sharing Helm Charts:
you need some deployment in your cluster, you can search
>Helm search <keyword>  or you can to Helm Hub ------>already existing Helms are present.
public and private repositories are there for Helm - private helms are related to organization shared within it.

2)Templating Engine:






#Metrics
kubectl top The kubectl top command returns current CPU and memory usage for a clusterâ€™s pods or nodes, or for a particular pod or node if specified.
  
  
  
  
  
 
  
  
  
  
  
    
 
  
  

