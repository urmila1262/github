What is Kubernetes?

Kubernetes is a container management or orchestration tool.It is also known as K8s, its a open source and written in GO language.
container orchestration tool or engine automates deploying,scaling and managing containerized application on a group of servers.
ex: Kubernetes,docker swarm,Apache Mesos Marathon.

Kubernetes --->take care of deploying,scaling,scheduling,load balancing,batch execution,rollbacks and monitoring.

1) Automatic bin packaging: Kubernetes automatically packages your application and schedules the container based on the requirements and resources available.
Pods and Nodes: containers are placed in functional unit called pod and pods are housed inside Nodes
2)Service discovery and load balancing:
How kurbernetes organizes containers
Kubernetes doesn't run containers directly instead it wraps one or more containers into a high-level structure called pod.Each pod contain a storage volume and 
every pod has a unique network IP.
All pods having same set of functions are abstracted into sets called services and this services has given a DNS name with this system Kubernetes has control over 
network and communication between pods and can load balance accross them.
3) Storage Orchestration:
Containers running in the pods need some storage but a single storage volume present in each pod for all containers.
Kerburnetes allows to mount the storage system of your choice like local,cloud(AWS),NFS(Network file system).
4) Self healing:
If a container fails Kubernetes restarts the container,if node dies - replaces and reschedule containers on other nodes,if container doesn't respond to user 
defined health check - kill container. The process that takes care of all this is replication controller.


## launch.sh --->it will launch the cluster
## kubectl cluster-info   --->it will give kubernetes master and DNS URL and port no(Health check)
## kubectl cluster-info dump  ---> to further debug and diagnose cluster problems
## minikube version
## minikube atart --wait=false
## kubectl cluster-info  --->cluster info
## kubectl get nodes -->to view nodes in the cluster.
## docker -v --->docker version
## kubectl run ----> it allows containers to be deployed onto the cluster
## kubectl create deployment first-deployment --image=katacoda/docker-http-server  ----->to create the deployment
## kubectl get pods  ----> to check status of deployment we use this command
## kubectl expose deployment first-deployment --port=80 --type=NodePort   ---->once the container is running it can be exposd via diff networking options, depending on requirements.
One possible solution is NodePort, that provides a dynamic port to a container.

## export PORT=$(kubectl get svc first-deployment -o go-template={{range.spec.ports}}{{if.nodeport}}{{"\n"}}{{end}}{{end}}')
## echo "Accessing host01:$PORT"
Accessing host01:30340
## curl host01:$PORT
<h1>This request was processed by host: first-deployment-666c48b44-4kb2r</h1>  ----------->All this above commands finds the allocated port and executes a HTTP request.

Enable the dashboard
## minikube addons enable dashboard
## Kubectl apply -f /opt/kubernetes-dashboard.yaml --->we will get msg like service/kubernetes-dashboard-katacoda created.


What is K8s?
Kubernetes is open source container orchestration tool developed by google helps you to manage containerized applications in different deployment environments 
like physical,virual or cloud machines.

What features does orchestration tool offer?
1)High availability or no downtime(always accessible by users)
2)Scalability or high performance
3)Disaster recovery - backup and restore

Terms in K8s:
Node:Node is a physical or virtual machine.

Pod:Smallest unit of k8s and abstraction over a container creates running environments over containers or images.Each pod gets its own IP Address and it is internal
    to communicate with other pods. Pods will die easily when containers,application or node is crushed and new pod will be created with new IP address.
    
Service:Service is having static IP Address and each pod has service, even if pod dies the service and its IP remains same.

Ingress:In Kubernetes, an Ingress is an object that allows access to your Kubernetes services from outside the Kubernetes cluster. You configure access by creating 
    a collection of rules that define which inbound connections reach which services. This lets you consolidate your routing rules into a single resource.(routing
    Traffic to pods).
    
ConfigMap: External configuration for your app like DB etc and it is connected to the pod so that when ever pod requires DB it will get from configmap. If there 
   is any change in name or URL of DB then simply change in configmap helps app to access new DB.
   
Secret:Secrets came into picture becoz we can't store pwd of DB in configmap, so in secrets we put our DB credentials and which is encoded in base64.Just like
  configmap we can also connect secret to the pod so that pod can access data in both by using environmental varables or as a properties file.
  
Volumes:If DB pod or service get restarted our data will gone so Volumes came into picture.Volume is physical storage on local machine or remote outside of k8's 
  cluster(may be cloud or your own storage).k8's doesn't maintain any data persistance.
  
Replica:Suppose if the pod crushes or restarts in production it will effect our operations so,we need to maintain replica of pods with same service and it also 
  helps in load balancing in operations.For this replica we can use blueprints of my-app pods this is deployments.
  
Deployment:We can deploy the pods for replica used for loadbalancing so we can scale up and down the replicas.

Statefulset:In case of DB pods it will replicate Storage space but not data in it and also we can access to read or retrive data so we use Statefulset for DB and 
  other stateful apps.(A stateful app is a program that saves client data from the activities of one session for use in the next session.)
  
  
  Kubernetes Architecture:
  
  One of the main components of Kubernetes is Node,each node has multiple pods running on it.
  Three processes that must be installed on every Node that should schedule and manage pods.Nodes are cluster servers that actually do work.
  
  First process that need to run on each node is
  
  1)Container runtime: (Docker or any other containers)You need to install a container runtime into each node in the cluster so that Pods can run there. 
    This page outlines what is involved and describes related tasks for setting up nodes.
    
  2)Kubelet: The kubelet is the primary "node agent" that runs on each node and it interacts with both container and node.Kubelet is responsible to run the 
    pod with a container inside by taking configuration and assigning resources from container to the pod like cpu,storage resources.
    
 3)Kube proxy:It will forwards the requests(communication between node)
    kube-proxy is a network proxy that runs on each node in your cluster, implementing part of the Kubernetes Service concept. 
    kube-proxy maintains network rules on nodes. These network rules allow network communication to your Pods from network sessions inside or outside 
    of your cluster.
    
 How do you interact with this cluster?
 How to:
    Schedule pod?
    Monitor?
    Reschedule/Restart pod?
    Join a new node?
  All above processes are controlled by Master processes which will take care of all the nodes.
  
  Four processes run on every master node that controls cluster state and worker nodes as well
  
  1)API server: To deploy a new application in a kubernetes cluster, interact with API server with some client.API server is like cluster gateway which gets 
  initial requestes for any updates into the cluster or any queries from the cluster and also it acts as a gate keeper authentication to make sure that an
  authorized request came to the cluster. When you want to Schedule new pods, deploy new apps, create new services you need to interact with API server on 
  the master node then API server validate your request if everything is fine it will forward your request to the other processes inorder to schedule pod
  or other components that you requested. It is good for security purpose becoz it has one entry point to the cluster.
  
  2)Scheduler: if you send request to create new pod the request will send through API server to Scheduler, Scheduler knows where to create pod(on which worker
  node) and how much CPU,memory required.Scheduler just decides on which node new pod should be scheduled, sends request to kubelet, it will do actual 
  configuration of CPU,memory etc and executes the process.
  
  3)Controller manager: Detects when pod dies or cluster state changes and send request to the scheduler again same process repeats it will check which node 
  is having more free space, then it will restart the pods or schedule to create new pods. 
  
  4)etcd: It is a key value store of cluster simply called as cluster brain,what ever the changes(suppose pod dies,new pod scheduled etc) occur in the cluster
  that will be stored in the etcd, but application data is not stored in the etcd. It will store only data related to cluster master node.
  
  Minikube:
  Minikube is a master node cluster where masternode and worker node both run on one node.In this node the docker container is pre-installed so you can run 
  containers or deploy the containers. It will run on your laptop as a virtualbox, basically creates virtualbox on your laptop the node runs on that virtual box.
  To summerize minicube is a one node cluster that run in a virtualbox on your laptop used to test kubernetes on local setup.
  
  Kubectl:
  Which is a command line tool for kubernetes cluster to create nodes and services.
  First we need to speak with master process and one of the master process in minikube is API server is actually main entry point to the kubernetes cluster.
  We need to talk to API server and the way to talk to API server are many way through UI,kubernets API,CLI called kubectl and kubectl is the most powerful
  among three clients. Kubectl used to create pods,create services and destroy pods etc.
  Kubectl is not only used to interact with minicube but also used for cloud clusters.
  
  >brew update
  >brew install hyperkit
  >brew install minikube
  >kubectl
  >minikube
  >minikube start --vm-driver=hyperkit --->saying minikube to use hyperkit hypervisor
  >kubectl get node --->gives status of running nodes
  >minikube status --->we can get status of minikube,kubectl,APIserver running or not and kubeconfig -configured.
  >kubectl version --->to know the version of client and server.
  >kubectl get deployment --->to show deployments
  >kubectl get pod --->status of pods
  >kubectl get services --->status of services or list any kubernetes components
  >kubectl create -h --->help for kubectl create command(list of components to create)
  Note: Pod is a smallest unit we are not creating pods directly but there is an abstraction layer over it called deployment.
  
  >kubectl create deployment NAME --image=image [--dry-run] [options]
  >kubectl create deployment nginx-depl --image=nginx --->creates latest nginx container from docker hub
  >kubeclt get deployment --->gives status of deployments.
  
  >kubectl get pod --->to see the pods with status container creating,running
  Note: Deployment has blueprint to create pod, most basic configuration for deployment(name and image to use)
  
  >kubectl get replicaset ---> this is another component which will come by default(Replicaset is managing the replicas of pods)
  Note: Pod will have the hash of replicaset and pods hash ID. We can't create,delete replicas just deploy, for more replicas we have options. 
  
  >kubectl edit deployment nginx-depl ---> directly editing pods(Auto generated configuration file with default values)
  Note: We can change in this configuration file in pods and save the changes it will update with new pod
  
  >kubectl get deployment ---> new deployment is available
  >kubectl get pod --->old(terminating) and new pods are shown but new pod is running.
  >kubectl get replicaset --->old one has no pods it in and new has pods
  >kubectl describe pod nginx-depl --->gives all details of the pod.
  
  Debugging pods:
  >kubectl logs nginx-depl-5c8bf76b5b-pn2kq --->gives the logs related to pods
  >kubectl exec -it {pod-name} -- bin/bash ---> To get the terminal of the pod(to get inside pod we will get shell - as admin), exit to come out from terminal.
  

  create mongo deployment
>kubectl create deployment mongo-depl --image=mongo
>kubectl logs mongo-depl-{pod-name}
>kubectl describe pod mongo-depl-{pod-name}

delete deplyoment
>kubectl delete deployment mongo-depl --->delete the pods mango db.
>kubectl delete deployment nginx-depl

create or edit config file

>vim nginx-deployment.yaml

-------
apiVersion: apps/v1
kind: Deployment   ----->what to create
metadata:
  name: nginx-deployment ---->name of the deployment
  labels:
    app: nginx 
spec:      ---->Specification of deployment
  replicas: 2 ---->how many replicas
  selector:
    matchLabels:
      app: nginx
  template:    ----->template has metadata and its own spec here pod is defined
    metadata:
      labels:
        app: nginx
    spec:      ------>specifications of pod(blueprint for the pods)
      containers:
      - name: nginx ----->name of 
        image: nginx:1.16
        ports:
        - containerPort: 8080 ----->on what port pod need to run

-----
>kubectl apply -f nginx-deployment.yaml ----> nginx deployment created.

>kubectl get pod ----->we can see pod created and running
>kubectl get deployment. ---->deployment created few secs ago
Note:We can change the file configuration if any extra replicas required, then again run apply command.

YMAL configuration file in kubernetes
overview:
-The three parts of configuration file
-Connecting deployments to service to pod
-Demo

3 parts are:

1) Metadata :Name of the component
2)Specification: every configuration is defined here(attributes inside spec are specific to kind -deployment or service).
first two lines gives us what is kind of component and apiversion will change for every deployment and service.
3)Status:Automatically generated and added by kubernets.k8s always compairs (desired state # actual state) if it doesn't match then k8s try to fix it becoz of
its self healing nature.It will check in config logs status and find any diff in status and specification it will adds status, updates frequently. k8s get this 
information from etcd. etcd holds the current status of any k8s components.


nginx-service.yaml

--------

apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  selector:
    app: nginx
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8080

--------

Formate of configuration file:

- .yaml extention 
- syntax: it is strict with syntax. (yaml validators online to fix syntax)
- store the config file with your code or to have its own git repository.

Connecting components:

Labels and Selectors
metadata part contains labels and spec part contains selectors

metadata -- any key value pair for componenet

-------
labels:
 app:nginx  ---> here we have nginx
-------
 
 -pods get the label through the template blueprint.
 -This label is matched by the selector
 
 -------
  selector:
   matchLabels:
    app:nginx
 -------

>kubectl apply -f nginx-deployment.yaml
>kubectl apply -f nginx-service.yaml
>kubectl get pod
>kubectl get service ---->it will shows services created and with the defined name,port etc.
>kubectl describe service nginx-service
Note:here we get info related to service and endpoints are also found where IP addresses of pods under that service are defined.
>kubectl get pod -o wide ----> here we get more info IP address also present.

Status Automatically generated:
>kubectl get deployment ngnix-deployment -o yaml ---->will get the updated configuration of deployment which result in the etcd of cluster for every component.
>kubectl get deployment ngnix-deployment -o yaml>nginx-deployment-result.yaml ---->saving output in the file

delete with config
>kubectl delete -f nginx-deployment.yaml ----->to delete the deployment.
>kubectl delete -f nginx-service.yaml

Complete Application setup with kubernetes components:
mangodb,mangoexpress
we can create mangodb pod and create internal service to the pod so that only internal requests take to the pod. 

Then we create mango express  
1)give URL of mango db so that mango express will connect to DB
2)mango db username and pwd so that mango express will authenticate to DB
Note: We can pass this data to mango express with deployment.yaml configuration file by declaring some variables. We have configmap which contains URLs and 
Secrets contains credentials we have referance for this in deployment file. Now mango express access this through browser in order to do this we have an external
service that allow external request to talk to the pod.
URL:
-Ip address of node
-Port of external service.

Browser Request flow through the k8s components:
Request starts from browser --->it will connect with mango express external service ---->sends it to mango express pod --->then internal service of mango DB
(db URL) --->then it will send to mango db pod where it will authenticate the request using credentials.

>kubectl get all --->it will get all components running in k8s

nodePort:must be in the range of 30000 to 32767 --->this port no is given in browser to access external service

>minikube service mongodb-express-service --->displays name,url with publice IP and external port as specified to open in browser.

Kubernetes namespaces:
- What is a Namespace?
- What are the use cases?
- How namespaces work and how to use it?

In k8s we can organise resources in namespaces, there are many namespaces in kubernetes cluster.We can say name space is virtual cluster inside cluster.
k8s provides default namespaces
>kubectl get namespace ---->gives names of default namespaces

5)kubernetes-dashboard is only with minikube it is not installed in cluster.
1)kube-system namespace --->is not ment for you basically you should not create or modify in kube-system. The components that are defined in kube-system are system
processes they are from master managing process or kubectl etc.
2)kube-public namespace contains publicly accessible data, it has a configmap which contains cluster information so it is accessible without authentication.
>kubectl cluster-info ---> gives info is control panel, kube DNS are running.
3)kube-node-lease ---> hold info of heartbeats of nodes, each node has assiociated lease object in namespace, determine the availability of a node.
4)Default namespace ---> To create resources at the begining if you haven't created a new name space. You can add and create new namespaces.

>kubectl create namespace my-namespace ---> to create namespace.
>kubectl get namespace --->we can see new namespace in the list.

what is the use of namespaces?

1)Create namespaces with configuration files:
If we create all our resources in default namespace it will be hard to find the specific resouces belongs to so, we can create our own namespaces to avoid the
confusion,ex:create db namespace and add all resources related to DB, like monitorning, elastic stack, nginx-ingress name space etc logically grouping the 
resources inside the cluster, it is not suggested for smaller projects contains lessthan 10 resources.

2)conflicts:
If two teams are using same cluster they can deploy same app so that the 2nd app deployment overwrite 1st app deployment. Inorder to avoid the conflicts 
in cluster give two namespaces for each team to aviod overwiting even if the app name is same. If they are using jenkins automation tool for deployment 
they not even knows they have overwritten.

3)Resource sharing: staging and deployment:
If two staging and deployment or (production blue and production green) are using same Nginx-ingress controller,Elastic stack then we can place in diff 
namespace and share them two both staging and deployment.

4)Limit resources and access to namespaces when you are working with diff namespaces:
If two teams are working on two diff namespaces in the same cluster, grant permissions only to the respective namespaces avoiding the teams to access and 
change other teams deployments.Each  team has it own,isolated environment.Limit CPU,RAM,Storage per namespaces.

Characteristics of namespaces?

1)You can't access most of the resources from another namespace. Each NS must have its configmap and secrets.
2)Service in another NS can be used by the other NS
3)Components, which can't be created within a NS, they live globally in a cluster and you can't isolate them like volume,node etc
>kubectl api-resources --namespaced=false ---> to list the isolate resources or not in NS in cluster.
>kubectl api-resources --namespaced=true --->resources in NS.

Create componenets in a NS:

------
mysql-configmap.yaml

apiVersion: v1
kind: ConfigMap
metadata:
  name: mysql-configmap
data:
  db_url: mysql-server.database
------

Note: If no NS is defined then the resource will be created in default NS.
>kubectl apply -f mysql-configmap.yaml
>kubectl get configmap
>kubectl get configmap -n my-namespace
>kubectl get all -n my-namespace  --->all resources in NS are listed
>kubectl apply -f mysql-configmap.yaml --namespace=my-namespace --->create configmap in the namespace
>kubectl get configmap -n my-namespace ----> to verify the resource in namespace 

Change active namespace:
To change from default NS to specific NS we need to install an app called "kubens".
>brew install kubectx ---> this can install kubens as well
>kubens ----> gives list of NS and highlight hte one which is active.
>kubens my-namespace ----> to switch the active NS to the specified NS.

Note: Now we can execute Kubectl commands directly without specifying NS as we have used before for default NS.Installation of kubens is differ for OS.


Kubernetes Ingress:

- What is ingress?
- Ingress YAML configuration
- When do you need ingress?
- Ingress Controller

Note: Now we are connecting with an external app through browser by giving IP ,port and http secure protocal, but we can do through "my-app ingress" component
from internal, external service send to ingress and ingress send request to internal service and eventually goes to pod.

exe:
type: LoadBalance
  nodePort: 35010
  
This is for external connection, by providing external port no:

With ingress:

---------
apiVersion: networking.k8s.io/b1betal
kind: Ingress
metadata:
  name: myapp-ingress
spec:
  rules:       --------->routing rules:this host must be forwarding the request to the internal service.User enter browser and ingress define mapping.
  - host: myapp.com   ---->Valid domain address, maps domain name to node's IP address, which is entrypoint.(the node will be inside or outside kubernetes cluster)
    http:   ------->this protocal is not from external path, this is for incoming request gets forwarded to internal service.
      paths:       ----->this is URL path http://domain/path what ever the path after domain that is mentioned here
      - backend:
          serviceName: myapp-internal-service    ----->this is internal service, once ingress maps this is redirected to internal service
          servicePort:8080
---------

How to configure ingress in your K8s cluster?
We need implimentation for ingress which is ingress controller which is pod or set of pods in K8s cluster that evalutes and process ingress rules. The above yaml
file is installed in ingress.

pod<--- app service <---Ingress <----ingress contorller pod

What is ingress controller?

Function of ingress controller is to evalute all rules that you are defined in your cluster to manage all the redirections. This will be the entrypoint to all 
the domains and sub-domains you configured and it will evaluate all the rules in your cluster there are more than 50 rules, which farwording rule applys for that
specific request. k8s Nginx Ingress controller is one of the important controller.

Environment on which your cluster runs:
Cloud service provider like AWS,google cloud ..etc this have out-of-box k8s solutions and have own virtualized load balancer.Here cloud load balancer is there
the external request first hits the load balancer and load balaner send request to ingress controller.

Advantage of cloud provider is you don't have to implement load balancer by yourself it is up and running you can send requset to k8s cluster.
If you runnung k8s cluster in bare metal env you need to configure some kind of entrypoints.Either inside of cluster or outside as separate server you need to 
provide entrypoint and one of those is external proxy server which can be a software or hardware solution that will take the role of load balancer and entrypoint
to your cluster.you have a separate server and you would give IP address and open ports to the request to be accepted and this proxy server will act as 
entrypoint to the k8s cluster and this is the only one server accessing from outside in k8s this is really good security practice.

So, all the request went to the proxy server from there send to ingress controller here it will choose which rule should be taken and whole internal farwording 
will happen.

Configure ingress in minikube:
1)install ingress controller in minikube
>minikube addons enable ingress  ----->this will automatically starts the k8s Nginx implementation of ingress controller 

With one simple command ingress controller will be installed in your cluster 
>kubectl get pod -n kube-system  ---->you will see nginx ingress controller running in your cluster
Now you can create ingress rule the controller can evaluate.
>kubectl get ns ---> to see the k8s dashboard(in minikube dashboard is already out-of-box exist)
>kubectl get all -n kubernetes-dashboard   ------>shows all the components in k8s dashboard

Ingress rule creation:
------
apiVersion: networking.k8s.io/v1betal
kind: Ingress
metadata:
  name: dashboard-ingress
  namespace: kubernetes-dashboard
spec:
  rules:
  - host: dashboard.com
    http:
      paths:
      - backend:
          serviceName: kubernetes-dashboard
          servicePort: 80
------

>kubectl apply -f dashboard-ingress.yaml ----->name of ingress, this command creates ingress
>kubectl get ingress -n kubernetes-dashboard ---->NS name,this command gives us the ingress information.
>kubectl get ingress -n kubernetes-dashboard --watch ---> this will give IPaddress of ingress

Note:We can map this IP address in etc/host file so the the IP address is accessable with some name (dashboard.com this is not specific but here it is host name)
so if we browse dashboard.com will get k8s dashboard.

>kubectl describe ingress dashboard-ingress -n kubernetes-dashboard
this command gives info of ingress here "default backend: default-http-backend:80" so in k8s there is no rule specific to take backend it will be take by default
backend service.

------
apiVersion: v1
kind: Service
metadata:
  name: default-http-backend  --->name of default
spec:
  selector:
    app: default-response-app
  ports:
    - protocol:TCP
      port: 80   ---->port of default backend
      targetPort: 8080
------

Defining multiple paths for same host:
For ex: google is one domain but have multiple services

------
apiVersion: networking.k8s.10/v1beta1
kind: Ingress
metadata:
  name: simple-fanout-example
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - host: myapp.com
    http:
      paths:
      - path: /analytics
        backend:
          serviceName: analytics-service
          srevicePort: 3000
      - path: /shopping
        backend:
          serviceName: shopping-service
          servicePort: 8080
          
------
Here we can access different services with the help of the url and ports assigned for that services.

Multiple sub-domains or domains:
-------
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: name-virtual-host-ingress
spec:
  rules:
  - host: analytics.myapp.com
    http:
      paths:
        backend:
          serviceName: analytics-service
          servicePort: 3000
  - host: shopping.myapp.com
    http:
      paths:
        backend:
          serviceName: shooping-service
          servicePort: 8080
-------
Helm - Package manager of K8s:

- Main concepts of Helm.
- Helm changes a lot between versions
  understanding the basic common principles and use cases when and how we use Helms.
  
What is Helm?

Helm has many functions 

1)is used as package manager for k8s
To package yaml files and distribute them in public and private repositories.
ex: you have deployed your app in k8s cluster and you want to deploy Elastic stack for logging additionally in cluster your app will use it to collect logs.
To deploy elastic stack in k8s cluster you need some services like stateful set,configMap,secrets,k8s user with permissions,services etc

Using Helm Charts:
- Bundle of yaml files.
- Create your own Helm charts with helm.
- Push them to Helm repository (so that others can use)
- Download and use existing ones (you can use helms created by others)

Note: commonly used deployments are like database apps,mongodb,elastic search,mysql,monitoring apps like promotheus which have complex helm charts, so for all
this apps helms are available already, so you can reuse that configuration which others have did this sharing of helm charts improved importance of Helm.

Sharing Helm Charts:
you need some deployment in your cluster, you can search
>Helm search <keyword>  or you can search in "Helm Hub" ------>already existing Helms are present.
public and private repositories are there for Helm - private helms are related to organization shared within it.

2)Templating Engine:

Imagine you have an application that is made up of multiple micro services and you are deploying all in K8s cluster

--------
apiVersion: v1
kind: Pod
metadata:
  name: my-app
spec:
  containers:
  - name: my-app-container
    image: my-app-image
    port: 9001
--------

Deployment and service configurations are almost same but only diff is the app name(or image name) and version.Without helm you would have to write
multiple yaml files for each microservices but with the help of helm 1)Define a common blueprint 2)Dynamic values are replaced by placeholders(this is dynamic)
this files are called as template files(down)

-------
apiVersion: v1
kind :Pod
metadata:
  name: {{ .Values.name }}
spec:
  containers:
  - name: {{ .Values.container.name }}
    image: {{ .Values.container.image }}
    port: {{ .Values.container.port }}
-------

This values are dynamic and this are defined in external values.yaml file separately

------
name: my-app
container:
  name: my-app-container
  image: my-app-image
  port:9001
------

object, which is created based on the values defined either via "values.yaml" file or with "--set flag" from command line, this is used in practical for CI/CD. 
In your build you can replace the values on the fly before deploying them.

3) Another usecase for Helm features:

Same applications acorss different environments
Ex:Suppose you want to deploy an application having multiple micro service in K8s cluster in different env like development,staging,production it will be 
difficult to deploy in each environment separately so you can create your own Helm chart that will have all required yaml files which your deployment needs
now you can redeploy the same deployment in various K8s clusters using one command.

Helm chart structure:

Tipically its like directory structure

-----
mychart/
  Chart.yaml
  values.yaml
  charts/
  templates/
  ...
-----

Top level mychart folder ---> name of chart
chart.yaml --->meta info about chart (name,version,list of dependencies etc)
values.yaml ----> values for the template files - default values that can be overwrite after.
charts folder ---> chart dependencies if this chart is depend on other charts then that charts are configured here
templates folder ---> actual template files

# helm install <chartname> ---> to deploy the yaml files into kubernetes template files will be filled with values from values.yaml
optionally some other files like Readme or licence file will be there in this folder.

Values injection into template files:

values.yaml
----
imageName: myapp
port: 8080
version: 1.0.0
----
default

# helm install --values=my-values.yaml <chartname>

my-values.yaml
----
version: 2.0.0
----
overrides values


result
----
imageName: myapp
port: 8080
version: 2.0.0
----
.values object

Instead of above we can define on command line
# helm install --set version=2.0.0
NOte: But here having my-values.yaml file is best procedure than command line.

Third feature:
Release Management:
helm versions are 2 vs 3

Helm version 2 comes in two parts:
1)Client (helm CLI)
2)Server (Tiller)

Whenever you run the helm client it sends requests to Tiller to run the k8s.
# helm install <chartname>
Tiller will execute the request and run the services required by using yaml files inside K8s.
Whenever you create or change deployment Tiller will stores copy of each configuration sent by client in the server. Thus it will keep track of all chart 
executions.


Revision        Request
1           Installed chart
2.          Upgraded to v 1.0.0
3.          Rolled back to 1 version

# helm install <chartname>
# helm upgrade <chartname>
# helm rollback <chartname>

--> Changes are applied to existing deployment instead of removing and creating new one. If there are any changes in configuration files, upgrade is wrong or 
yaml files are wrong simply rolled back to privious version.
--> Handling rollbacks.
--> All the excution history is stored by Tiller when ever you send request from helm client to Tiller.

Downsides of Tiller:
1) Tiller has too much power inside of K8s cluster (like update,create delete etc - more permissions)
2) Big Security issue, hence in helm 3 version Tiller has removed.
3) Solves the security concern.

K8s volumes explained:

How to persist data in kuberneties using volumes?
Three components of K8s storage:
Persistent volume
Persistent volume Claim
Storage class

The need for volumes
For myapp application mysql DB is using and data gets added,updated in db until db pod exists, once it is restarted or crash then along with pod data will be gone.
. So, we need storage that doesn't depend on the pod lifecycle which is outside of K8s.
. Storage must be available on all nodes becoz we don't know on which node the new pod will be created.
. Storage need ot servive even if the cluster crashes.

Another use case for persistent storage:
This is for directories may be you have an app the can reads/writes the files(session files,config fils etc) from a dir, you can configure any of this type of 
data by using K8s persistent volumes. The persistent volume is like cluster resource that uses CPU or RAM for storage.
Persistent volume can be created like any other k8s components with yaml files but need to give kind as PersistentVolume and in spec need to give various 
parameters like storage,filesystem,permissions etc.

----
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-name
spec:
  capacity:
    storage: 5Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  storageClassName: slow
  mountOptions:
    - hard
    - nfsvers=4.0
  nfs:
    path: /dir/path/on/nfs/server
    server: nfs-server-ip-address
----

This needs actual physical storage, like local disk inside local host,nfs(network file system) server outside cluster or cloud storage like AWS,google cloud etc

Where does this storage comes from and who makes it available to the cluster?
K8s do not take care of all this we need to know what type of storage do we need?, and we need to create and manage them by ourselves.
Think the storage is as a external plugin to your cluster may be local disk,nfs server or cloud, so our apps inside cluster can link with local,remote or cloud
storages to retrive data.

Persistent volume yaml example:

Use this physical storages in the spec section
Ex of NFS storage as a backend:
----
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-name
spec:
  capacity:
    storage: 5Gi         --->how much storage
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce                           ------->storage parameters- permissions etc
  persistentVolumeReclaimPolicy: Recycle
  storageClassName: slow
  mountOptions:
    - hard
    - nfsvers=4.0
  nfs:
    path: /dir/path/on/nfs/server                         ---->NFS parametes for backup
    server: nfs-server-ip-address
----

EX of Google cloud as a backend:
----
apiVersion: v1
kind: PersistentVolume
metadata:
  name: test-volume
  labels:
    failure-domain.beta.kubernetes.io/zone: us-central1-a__us-central1-b
spec:
  capacity:
    storage: 400Gi       ---->how much storage
  accessModes:
  - ReadWriteOnce
  gcePersistentDisk:
    pdName: my-data-disk -----> backup
    fsType: ext4
----

Depending on storage type and backend, some of the spec attributes may vary.

Ex of local storage:
----
apiVersion: v1
kind: PersistentVolume
metadata:
  name: example-pv
spec:
  capacity:
    storage: 100Gi
  volumeMode: Filesystem
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Delete
  storageClassName: local-storage
  local:
    path: /mnt/disks/ssd1
  nodeAffinity:               ----> node affinity is extra here
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - example-node
----

Complete list of storage backends supported by K8s--- nearly 25 different storage backends that k8s supprot.

Note: Persistent volumes are not Namespaced but it is accessible to whole cluster.

Local vs Remote volume types:

1) Each volume type has it's own usecase
2) Local volume types violate 2. and 3. requirement of data persistance for data bases
    . Being tied to 1 specific node
    . Serviving cluster crashes     ----->becoz of this two drawbacks for DB persistence you should always use remote storage.
    
K8s Administrator and K8s user:

Who create the persistent volumes and when?
PV are resources like CPU,RAM  that need to be there before in the cluster when the pod that depends on it is created.

K8s Admin sets up and maintains the cluster and also he will take care of enough resources in K8s cluster (system admins,DevOps engineers in a company).
K8s user deploys applications in the cluster directly or by using CI/CD pipelines. This are developer DevOps Team who can develop the application and deploy.

1) Storage resources can be provisioned by Admin in remote or cloud that will be available for the cluster.
2) Creates the PV components form these storage backends.

Based on Developers storage, what type of storage there apps need, develpoers knows what type of application and storage there applications need.

Persistent volume claim component:

Developer know that there is volume storage and how to use it, for this developers need to explictly configure the apps  yaml file to use those persistent 
volume components. Another thing Application has to claim the persistent volume and to do that using another component of K8s called Persistent volume claim. 

Presistent volume claim or PVC are also created by yaml files
ex:
----
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: pvc-name
spec:
  storageClassName: manual
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
----

PVC claims the volume with some storage capacity which is defined the persistent claim and in addition to that it will restrict like permissions(400,741, etc)
what ever the criteria matches it will use that. We have to use that PVC in the pods configuration.

-----
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:      ----->Volume is mounted into the container.
    - name: myfrontend
      image: nginx
      volumeMounts:
      - mountPath: "/var/www/html"    ----> Apps can access the mounted data here "/var/www/html"
        name: mypd
  volumes:           ----> Here volume is mounted into the pod.
    - name: mypd
      persistentVolumeClaim:
        claimName: pvc-name
-----

Here claim has configured in pods configuration, so all the pods and services inside the pod can access the PVC.

Levels of volume abstractions:
. Pod requests the volume through the PV claim
. Claim tries to find a volume in cluster (based on the storage and permissions etc)
. Volume has the actual storage backend (in this way volumes gets the backend)
. Claims must be in the same namespaces where pod exists.

The container will read and write the volumes and if the container dies the new container will be created in its place and it have access to the same volume.

Why so many abstrictions to use volumes?
Admin provisions storage resource.
User creates claim to PV.

ConfigMap and secrets:
- local volumes
- not created via PV and PVC
- managed by Kubernetes

Consider you need a configuration file for your pod or certificate file for your pod in both cases you need a file available to your pod to do this 
1) Create ConfigMap and/or Secret component.
2) Mount that into your pod/container.

----
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
    - name: busybox-container
      image: busybox
      volumeMounts:
        - name: config-dir
          mountPath: /etc/config
  volumes:
    - name: config-dir
      configMap:        -----> like PVC here we use configMap
        name: bb-configmap
----

What we have covered so far?
Volume is directory with some data.
This volumes are accessible to the containers in a pod.
How the dirs made available and storage medium backup that and contents are defined by specific volume types 


Different volume types in pod:
Lets say elastic-app,configMap,secrets,needs db storage lets say awsElasticBlock Store

----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: elastic
spec:
  selector:
    matchLabels:
      app: elastic
  template:
    metadata:
      labels:
        app: elastic
    spec:                             ----> pod specifications.
      containers:
      - image: elastic:latest
        name: elastic-container
        ports:
        - containerPort: 9200
        volumeMounts:
        - name: es-persistent-storage
          mountPath: /var/lib/data
        - name: es-secret-dir
          mountPath: /var/lib/secret
        - name: es-config-dir
          mountPath: /var/lib/config
      volumes:                          -----> here at volumes level you can specify all the voulmes you want to mount 
      - name: es-persistent-storage
        persistentVolumeClaim:
          claimName: es-pv-claim
      - name: es-secret-dir
        secret:                        ----> here we have secrets
          secretName: es-secret 
      - name: es-config-dir
        configMap:                     ----> here is configMap
          name: es-config-map
----

Storage class:
1) Admins configure storage
2) Create persistent volumes
3) k8s users claim PV using PVC

Note: In the cluster there are many apps that need storage for which PVs are required so developers request the admins to create PVs from the cloud
that requires storage manual this is time consuming.

To make this process efficiently there is 3rd component in K8s, which makes process more efficient called Storage class.
Storage class provisions persistent volumes dynamically, when the PersistentVolumeClaims claims it. This way creating or provisioning of cluster has 
automated.
Storage class also gets created with yaml files.

----
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: storage-class-name
provisioner: kubernetes.io/aws-ebs
parameters:
  type: io1
  iopsPerGB: "10"
  fsType: ext4
----
 kind: StorageClass --->storage class creates persistent volumes dynamically
 storage backend is defined in the SC component  - via "provisioner" attribute. this will tell K8s which provisioner should be used for storage plotform
 or cloud provider to create the persitent volume automatically. Each storage backend has its own provisioner - internal provisioner - "kubernetes.io"
 and other storage type ther are external provisioner, we can also define parameters along with provisioner.
 
 Storage class is another abstraction level that abstracts underlying storage provider as well as parameters for that storage.
 
 How do we use storage class in the pod parameters?
 Same like persistent volume it is requested by PVC
 
 PVC config:
 ----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
     name: mypvc
spec:
     accessModes:
     - ReadWriteOnce
     resources:
       requests:
         storage: 100Gi
     storageClassName: storage-class-name
 ----
 
 
 Storage class config:
 ----
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: storage-class-name
provisioner: kubernetes.io/aws-ebs
parameters:
  type: io1
  iopsPerGB: "10"
  fsType: ext4
 ----
 
 1) pod claims storage via PVC
 2) PVC requests storage from SC
 3) SC creates the PV that meets the need of the claims and for storage backups.
 
 K8s atatefulset explained:
 
What is statefulset?
In k8s component that is used specifically for stateful applications.

What is stateful application?
EX: All DBs mysql,mongoDB,elasticsearch etc.
Any application that stores data and keeps track of its state, in other words this are apps that keeps track of data by saving it in some storage.

Stateless applications in other words do not keep track of previous state in each request it is completely new isolated interaction. Some time stateless apps
connect to the stateful apps to forward those requests.

Deployment of stateful and stateless applications:

Stateless apps are deployed using deployment which will replicate your pod (1,2 ...)
Stateful apps are deployed using statefulset, just like deployment it will replicate the stateful apps,pods to create mltiple stateful apps.
Both managed pods based on container Specification, configure storage with both of them equally in same way so both of them are doing same thing.

What is difference b/w Deployment and statefulset:

- Replicating stateful apps is more difficult it needs other requirements.
EX:Suppose we have the mysql db and java app
here java apps can be replicated it is pretty easy (identical and interchangable)  but to replicate mysql we need again java app for all replicas along 
with mysql.
Deployment can creates pods in random order with random hashes, and one service that can load balances to any pod. Even when delete it can be deleted in 
random order by choosing or in the way how they have created.

other hand mysql:
Its more difficult to create/delete at the same time and can't be randomly addressed becoz replica pods are not identical (pod identity)

Pod identity:
- sticky identity for each pod. 
- created from same specification, but not interchangable.
- persistent identifier across any re-scheduling if pod dies it will be recreated but it will keeps the previous identity.

Why is this identity necessary?
Suppose there is one mysql db which will access the volume storage it can read and write, but all other mysql pods don't have write permissions becoz of 
data inconsistancy issue still they have read only access. The pod with read,write permissions is called "Master Pod" and all others are called 
"slave pods" or "worker pods"
The Slaves do not use the same physical storage, they will have replicas of the storage as well so they are continuously synchronize the data
but master only updates the data and worker must know about each change to be up-to-date for the next query request. If new pod created then it should
have storage it will clone first and and synchronize from then. But data will be lost if all pods dies at a time when cluster crashes so persistent data 
will be used atleast for stateful apps, data will servive even if all pods dies. Persistent volume lifecycle is not tied to other component's lifecycle.

Pod state:
Here it will contain info as this is master or slave so even if pod dies it identify the state of the pod but here remote storage using will be helpful
than local.

Pod identify:
Every pod has its own identifier. unlike deployment: random hashes but statefulset:fixed ordered names
$(statefulset name)-$(ordinal) --- mysql-0(master),mysql-1(slave),mysql-2(slave) etc
Note: Next pod is only created, if previous is up and running, and if the previous is pending then it will wait to create the next pod.

Delete statefulset or scale down it to one replica, deletion is in reverse order first last one is deleted. All this is in order to protect data and state
of stateful app.

2 pod endpoints:

Each pod will gets it own DNS endpoint from service. 
loadbalancer service  --- same as deployment apart from that it has individual service(DNS) name for pods
individual service name is like ${pod.name}.${governing service domain}

1) Predictable pod name --- mysql-0
2) Fixed individual DNS name --- mysql-0.svc2
when pod restarts: IP address changes, name and endpoint stays same

sticky identity: retain state,retain role

Note: Stateful apps not perfect for containerized environments but it is good for stateless apps


K8s Services:
What is a service and when we need it?
Each pod has its own IP address, pods are ephemeral - destroyed frequently and creates new pod with new IP address. So, service came into picture 
it has stable IP address and each pod is with one service and also help for load balancing, service are loose coupling, communicate easily within or 
outside k8s cluster.

1) ClusterIP Services:
This is the default type even you forgot to give service it will take the ClusterIP service.
Ex: microservice app deployed(running on pod - 3000 port) in cluster and a side-car container(9000 port suppose) which will collects microservice logs to db.
and pod also gets IP address from Node's range.

To see the IP address of Pod # kubectl get pod -o wide (o is o/p)

Suppose we can connect to this pods from browser, first ingress will talk to the service which has an IP and port and now service will sends that request to 
one of the pod.

How the service knows to which pod and to which port request need to send?

To which pods:

this will select the pod with the help of "selector", 
- pods are identifed via selectors.
- key value pairs
- labels of pods
- random label names. ---> this we can specify in yaml file.

If three replicas are there under the service with same label all three will be matched by service.
- service matches all replicas
- registers as endpoints
- must match all the selectors 

To which port:

It is defined in targetPort attributes.

Service endpoints:

# kubectl get endpoints

K8s creates endpoint objects
- same name as service
- keeps track of which pods are the members/endpoints of the service.

Service port is arbitrary but targetPort must match the port, the container is listening at. Now the external service got the connection to the pod and now 
need to connect to the DB mysql so pod sends request to service to get the DB pod at a particular port .

Multi-Port services:

- Second container running for monitoring metrics (mongo-db app,mongo-db exporter) so when you are defining multiple names define the ports for each.


Headless Service:
- client wants to communicate with one specific node directly.
- Pods want to talk directly with specific pod
- so, not randomly selected.
- usecase: Stateful apps, like databases, pod replicas are not identical. Here direct communication needs for individual pods.

- client needs to figure out IP addresses of each pod.
But to get this option-1: API call to K8s API server
- Makes app too tied to k8s API
- inefficient. ---> this two are drawbacks.
Option-2: DNS lookup 
- DNS Lookup for service - return single IP address(cluster IP)
- Set ClusterIP to "None" - returns Pod Ip address instead
Note: Basically we can configure clusterIP: None for headless service. No cluster IP address is assigned.
# kubectl get svc ---> gives list of services

3 service type attributes:

ClusterIP:
---
apiVersion: V1
kind: Service
metadata:
  name: my-service
Spec:
  type: ClusterIP
---
- Default, type not needed
- internal service


Node port:
---
apiVersion: V1
kind: Service
metadata:
  name: my-service
Spec:
  type: NodePort
---
- Cluster IP is accessable only inside cluster but with Node port external traffic has to access to fixed or static port on each worker node. 

LoadBalancer:
---
apiVersion: V1
kind: Service
metadata:
  name: my-service
Spec:
  type: LoadBalancer
---


#Metrics
kubectl top - The kubectl top command returns current CPU and memory usage for a clusters pods or nodes, or for a particular pod or node if specified.
  
  
  
  
  
 
  
  
  
  
  
    
 
  
  

